{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.vf import VFEstimator\n",
    "from model.textencoder import TextEncoder\n",
    "import torch\n",
    "import librosa\n",
    "from utils.feature import TorchAudioFbank, TorchAudioFbankConfig\n",
    "from tokenizerown import LibriTTSTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sample_rate = 24000\n",
    "n_mels      = 100\n",
    "d_model     = 256\n",
    "depth       = 5\n",
    "num_heads   = 4\n",
    "downsample_factors = [1, 2, 4, 2, 1]\n",
    "\n",
    "text_encoder = TextEncoder(vocab_size=160, emb_dim=128).to(device)\n",
    "vf_estimator = VFEstimator(dim_in=n_mels, dim_model=d_model, conv_hidden=1024, num_heads=num_heads, Nm=depth, downsample_factors=downsample_factors).to(device)\n",
    "\n",
    "ref_audio_path = './test.wav'\n",
    "script = \"Hello, world!\"\n",
    "\n",
    "tokenizer = LibriTTSTokenizer(\n",
    "    special_tokens=[\"<filler>\"],\n",
    "    token_file=\"./vocab_small.txt\",\n",
    "    lowercase=True,\n",
    "    oov_policy=\"skip\",        # OOV은 버림 (또는 \"use_unk\", \"error\")\n",
    "    unk_token=\"[UNK]\",        # oov_policy=\"use_unk\"일 때만 필요\n",
    ")\n",
    "fbank = TorchAudioFbank(config=TorchAudioFbankConfig(sampling_rate=sample_rate, n_mels=n_mels, n_fft=1024, hop_length=256))\n",
    "\n",
    "audio, sr = librosa.load(ref_audio_path, sr=24000, mono=True)\n",
    "audio = torch.from_numpy(audio)\n",
    "logmel = fbank.extract(audio, sr).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b134d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(ref_audio_path, sr=24000, mono=True)\n",
    "audio = torch.from_numpy(audio)\n",
    "logmel = fbank.extract(audio, sr).unsqueeze(0)\n",
    "print(\"logmel : \", logmel.shape)\n",
    "B, T_ref, _ = logmel.shape\n",
    "\n",
    "token_ids = tokenizer.texts_to_token_ids([script])\n",
    "token_ids = torch.tensor(token_ids, device=device)\n",
    "print(\"token_ids : \", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb_128 = text_encoder(token_ids)\n",
    "print(\"text_emb_128 : \", text_emb_128.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_latents = torch.randn(B, 100*4, n_mels, device=device)\n",
    "time_t = torch.rand((B,), device=device)\n",
    "print(\"time_t : \", time_t, time_t.shape)\n",
    "\n",
    "output = vf_estimator(noisy_latents, time_t, text_emb_128)\n",
    "print(\"output : \", output.shape) # B, secs*100, n_mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in vf_estimator.parameters())\n",
    "print(f\"전체 파라미터 수: {total_params:,}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in text_encoder.parameters())\n",
    "print(f\"전체 파라미터 수: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085a073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc1bda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khj6051/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11775\n",
      "1309\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"atmansingh/ljspeech\")\n",
    "print(len(ds['train']))\n",
    "print(len(ds['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08277ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.text_mel_datamodule import TextMelDataset, TextMelDataModule\n",
    "\n",
    "dm = TextMelDataModule(\n",
    "    name=\"ljspeech\",\n",
    "    dataset=ds,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    n_spks=1,             # LJSpeech = 단일 화자 → 1\n",
    "    n_fft=1024,\n",
    "    n_feats=100,           # mel bins\n",
    "    sample_rate=24000,\n",
    "    hop_length=256,\n",
    "    f_min=0,\n",
    "    f_max=8000,\n",
    "    data_statistics={\"mel_mean\": 0.0, \"mel_std\": 1.0},\n",
    "    seed=42,\n",
    "    load_durations=False, # alignment 정보 필요 없으면 False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386bc263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage ;  0\n"
     ]
    }
   ],
   "source": [
    "dm.setup(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13778207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khj6051/star/data/text_mel_datamodule.py:194: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"x\": torch.tensor(token_ids), \"y\": torch.tensor(mel), \"durations\":None, \"text\": data['text']}\n",
      "/home/khj6051/star/data/text_mel_datamodule.py:194: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"x\": torch.tensor(token_ids), \"y\": torch.tensor(mel), \"durations\":None, \"text\": data['text']}\n",
      "/home/khj6051/star/data/text_mel_datamodule.py:194: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"x\": torch.tensor(token_ids), \"y\": torch.tensor(mel), \"durations\":None, \"text\": data['text']}\n",
      "/home/khj6051/star/data/text_mel_datamodule.py:194: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"x\": torch.tensor(token_ids), \"y\": torch.tensor(mel), \"durations\":None, \"text\": data['text']}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 147]) torch.Size([16, 940, 100])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = next(iter(dm.train_dataloader()))\n",
    "\n",
    "print(data['text'].shape, data['audio'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c615e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ae17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.module import TTSModule\n",
    "\n",
    "model = TTSModule(\n",
    "    dim=256,\n",
    "    depth=5,\n",
    "    num_heads=4,\n",
    "    attn_dropout=0.0,\n",
    "    ff_dropout=0.0,\n",
    "    min_span=10,\n",
    "    voco_type='vocos',\n",
    "    sample_rate=24000,\n",
    "    max_audio_len=2000,\n",
    "    optimizer = \"AdamW\",\n",
    "    lr = 1e-4,\n",
    "    scheduler = \"linear_warmup_decay\",\n",
    "    use_torchode = True,\n",
    "    torchdiffeq_ode_method = \"midpoint\",\n",
    "    torchode_method_klass = \"tsit5\",\n",
    "    max_steps = 1_000_000,\n",
    "    n_mels = 100,\n",
    "    text_emb_dim = 128,\n",
    "    downsample_factors = [1, 2, 4, 2, 1],\n",
    "    # 추가 하이퍼파라미터(옵션)\n",
    "    warmup_ratio = 0.05,\n",
    "    min_lr_ratio = 0.1,\n",
    "    weight_decay = 0.01,\n",
    "    betas = (0.9, 0.95),\n",
    "    grad_clip_val = 1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['text']\n",
    "audio = data['audio']\n",
    "text_mask = data['text_mask']\n",
    "audio_mask = data['audio_mask']\n",
    "\n",
    "print(text.shape, audio.shape, text_mask.shape, audio_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(text, audio, text_mask, audio_mask)\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "model.solve(text.to(\"cuda\"), audio.to(\"cuda\"), text_mask.to(\"cuda\"), audio_mask.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(text, audio, text_mask, audio_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from einops import repeat\n",
    "\n",
    "embed = nn.Embedding(160, 128).to('cuda')\n",
    "text_ids = torch.randint(0, 160, (1, 10)).int().to('cuda')\n",
    "text_ids = repeat(text_ids, \"1 n -> b n\", b=8)\n",
    "print(text_ids.shape)\n",
    "print(embed(text_ids).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13576b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2568b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.linspace(0, 1, 64, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d063257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a80679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from vocos import Vocos\n",
    "import torch\n",
    "\n",
    "def load_vocoder(is_local=False, local_path=\"\", device='cuda', hf_cache_dir=None):\n",
    "    # vocoder = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\").to(device)\n",
    "    if is_local:\n",
    "        print(f\"Load vocos from local path {local_path}\")\n",
    "        config_path = f\"{local_path}/config.yaml\"\n",
    "        model_path = f\"{local_path}/pytorch_model.bin\"\n",
    "    else:\n",
    "        print(\"Download Vocos from huggingface charactr/vocos-mel-24khz\")\n",
    "        repo_id = \"charactr/vocos-mel-24khz\"\n",
    "        config_path = hf_hub_download(repo_id=repo_id, cache_dir=hf_cache_dir, filename=\"config.yaml\")\n",
    "        model_path = hf_hub_download(repo_id=repo_id, cache_dir=hf_cache_dir, filename=\"pytorch_model.bin\")\n",
    "    vocoder = Vocos.from_hparams(config_path)\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n",
    "    from vocos.feature_extractors import EncodecFeatures\n",
    "\n",
    "    if isinstance(vocoder.feature_extractor, EncodecFeatures):\n",
    "        encodec_parameters = {\n",
    "            \"feature_extractor.encodec.\" + key: value\n",
    "            for key, value in vocoder.feature_extractor.encodec.state_dict().items()\n",
    "        }\n",
    "        state_dict.update(encodec_parameters)\n",
    "    vocoder.load_state_dict(state_dict)\n",
    "    vocoder = vocoder.eval().to(device)\n",
    "    return vocoder\n",
    "\n",
    "vocoder = load_vocoder(vocoder_name=\"vocos\", is_local=False, local_path=\"\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd16b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.feature import TorchAudioFbank, TorchAudioFbankConfig\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "fbank = TorchAudioFbank(config=TorchAudioFbankConfig(sampling_rate=24000, n_mels=100, n_fft=1024, hop_length=256))\n",
    "\n",
    "ref_audio_path='./test.wav'\n",
    "audio, sr = librosa.load(ref_audio_path, sr=24000, mono=True)\n",
    "print(audio.shape)\n",
    "audio = torch.from_numpy(audio)\n",
    "logmel = fbank.extract(audio, sr).unsqueeze(0)\n",
    "print(logmel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2aa15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav = vocoder.decode(logmel.to('cuda').permute(0, 2, 1))\n",
    "print(wav.shape)\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "display(Audio(wav.cpu().numpy(), rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd45f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hifigan.config import v1\n",
    "from hifigan.denoiser import Denoiser\n",
    "from hifigan.env import AttrDict\n",
    "from hifigan.models import Generator as HiFiGAN\n",
    "\n",
    "def load_vocoder(checkpoint_path):\n",
    "    h = AttrDict(v1)\n",
    "    hifigan = HiFiGAN(h).to('cuda')\n",
    "    hifigan.load_state_dict(torch.load(checkpoint_path, map_location='cuda')['generator'])\n",
    "    _ = hifigan.eval()\n",
    "    hifigan.remove_weight_norm()\n",
    "    return hifigan\n",
    "\n",
    "voco = load_vocoder('./generator_v1')\n",
    "denoiser = Denoiser(voco, mode='zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.inference_mode()\n",
    "def to_waveform(mel, vocoder):\n",
    "    audio = vocoder(mel).clamp(-1, 1)\n",
    "    audio = denoiser(audio.squeeze(0), strength=0.00025).cpu().squeeze()\n",
    "    return audio.cpu().squeeze()\n",
    "\n",
    "to_waveform(logmel, vocoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146c581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
