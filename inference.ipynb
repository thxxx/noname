{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daee7997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khj6051/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from newmodel.vf import VFEstimator\n",
    "from newmodel.textencoder import TextEncoder\n",
    "import torch\n",
    "import librosa\n",
    "from utils.feature import TorchAudioFbank, TorchAudioFbankConfig\n",
    "from tokenizerown import LibriTTSTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sample_rate = 24000\n",
    "n_mels      = 100\n",
    "d_model     = 256\n",
    "depth       = 5\n",
    "num_heads   = 4\n",
    "downsample_factors = [1, 2, 4, 2, 1]\n",
    "\n",
    "text_encoder = TextEncoder(vocab_size=160, emb_dim=128).to(device)\n",
    "vf_estimator = VFEstimator(dim_in=n_mels, dim_model=d_model, conv_hidden=1024, num_heads=num_heads, Nm=depth, downsample_factors=downsample_factors).to(device)\n",
    "\n",
    "ref_audio_path = './test.wav'\n",
    "script = \"Hello, world!\"\n",
    "\n",
    "tokenizer = LibriTTSTokenizer(\n",
    "    special_tokens=[\"<filler>\"],\n",
    "    token_file=\"./vocab_small.txt\",\n",
    "    lowercase=True,\n",
    "    oov_policy=\"skip\",        # OOV은 버림 (또는 \"use_unk\", \"error\")\n",
    "    unk_token=\"[UNK]\",        # oov_policy=\"use_unk\"일 때만 필요\n",
    ")\n",
    "fbank = TorchAudioFbank(config=TorchAudioFbankConfig(sampling_rate=sample_rate, n_mels=n_mels, n_fft=1024, hop_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b134d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(ref_audio_path, sr=24000, mono=True)\n",
    "audio = torch.from_numpy(audio)\n",
    "logmel = fbank.extract(audio, sr).unsqueeze(0)\n",
    "print(\"logmel : \", logmel.shape)\n",
    "B, T_ref, _ = logmel.shape\n",
    "\n",
    "token_ids = tokenizer.texts_to_token_ids([script])\n",
    "token_ids = torch.tensor(token_ids, device=device)\n",
    "print(\"token_ids : \", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb_128 = text_encoder(token_ids)\n",
    "print(\"text_emb_128 : \", text_emb_128.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_latents = torch.randn(B, 100*4, n_mels, device=device)\n",
    "time_t = torch.rand((B,), device=device)\n",
    "print(\"time_t : \", time_t, time_t.shape)\n",
    "\n",
    "output = vf_estimator(noisy_latents, time_t, text_emb_128)\n",
    "print(\"output : \", output.shape) # B, secs*100, n_mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in vf_estimator.parameters())\n",
    "print(f\"전체 파라미터 수: {total_params:,}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in text_encoder.parameters())\n",
    "print(f\"전체 파라미터 수: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085a073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc1bda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11775\n",
      "1309\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"atmansingh/ljspeech\")\n",
    "print(len(ds['train']))\n",
    "print(len(ds['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08277ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.text_mel_datamodule import TextMelDataset, TextMelDataModule\n",
    "\n",
    "dm = TextMelDataModule(\n",
    "    name=\"ljspeech\",\n",
    "    dataset=ds,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    n_spks=1,             # LJSpeech = 단일 화자 → 1\n",
    "    n_fft=1024,\n",
    "    n_feats=100,           # mel bins\n",
    "    sample_rate=22050,\n",
    "    hop_length=256,\n",
    "    f_min=0,\n",
    "    f_max=8000,\n",
    "    data_statistics={\"mel_mean\": 0.0, \"mel_std\": 1.0},\n",
    "    seed=42,\n",
    "    load_durations=False, # alignment 정보 필요 없으면 False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386bc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcafd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khj6051/star/data/text_mel_datamodule.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"x\": torch.tensor(token_ids), \"y\": torch.tensor(mel).transpose(1, 0), \"filepath\": \"filepath\", \"durations\":None}\n",
      "/home/khj6051/star/data/text_mel_datamodule.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"x\": torch.tensor(token_ids), \"y\": torch.tensor(mel).transpose(1, 0), \"filepath\": \"filepath\", \"durations\":None}\n",
      "/home/khj6051/star/data/text_mel_datamodule.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"x\": torch.tensor(token_ids), \"y\": torch.tensor(mel).transpose(1, 0), \"filepath\": \"filepath\", \"durations\":None}\n",
      "/home/khj6051/star/data/text_mel_datamodule.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"x\": torch.tensor(token_ids), \"y\": torch.tensor(mel).transpose(1, 0), \"filepath\": \"filepath\", \"durations\":None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 163]) torch.Size([16, 100, 846])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(dm.train_dataloader()))\n",
    "\n",
    "print(data['x'].shape, data['y'].shape)\n",
    "print(''.join(tokenizer.token_ids_to_tokens(data['x'][0].unsqueeze(0).tolist())[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112b23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ae17c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
