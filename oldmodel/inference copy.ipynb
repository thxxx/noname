{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daee7997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khj6051/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.vf import VFEstimator\n",
    "from model.refencoder import ReferenceEncoder\n",
    "from model.textencoder import TextEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import torch\n",
    "from utils.feature import TorchAudioFbank, TorchAudioFbankConfig\n",
    "from tokenizerown import LibriTTSTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sample_rate = 24000\n",
    "n_mels = 100\n",
    "d_model = 256\n",
    "\n",
    "ref_encoder = ReferenceEncoder(in_dim=n_mels, num_heads=4).to(device)\n",
    "text_encoder = TextEncoder(vocab_size=160, emb_dim=128).to(device)\n",
    "vf_estimator = VFEstimator(dim_in=n_mels, dim_model=d_model, conv_hidden=1024, num_heads=4, Nm=4).to(device)\n",
    "\n",
    "ref_audio_path = './test.wav'\n",
    "script = \"Hello, world!\"\n",
    "\n",
    "tokenizer = LibriTTSTokenizer(\n",
    "    special_tokens=[\"<filler>\"],\n",
    "    token_file=\"./vocab_small.txt\",\n",
    "    lowercase=True,\n",
    "    oov_policy=\"skip\",        # OOV은 버림 (또는 \"use_unk\", \"error\")\n",
    "    unk_token=\"[UNK]\",        # oov_policy=\"use_unk\"일 때만 필요\n",
    ")\n",
    "fbank = TorchAudioFbank(config=TorchAudioFbankConfig(sampling_rate=sample_rate, n_mels=n_mels, n_fft=1024, hop_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b134d7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logmel :  torch.Size([1, 301, 100])\n",
      "token_ids :  tensor([[20, 18, 24, 24, 27,  8,  3, 35, 27, 30, 24, 17,  4]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "audio, sr = librosa.load(ref_audio_path, sr=24000, mono=True)\n",
    "audio = torch.from_numpy(audio)\n",
    "logmel = fbank.extract(audio, sr).unsqueeze(0)\n",
    "print(\"logmel : \", logmel.shape)\n",
    "B, T_ref, _ = logmel.shape\n",
    "\n",
    "token_ids = tokenizer.texts_to_token_ids([script])\n",
    "token_ids = torch.tensor(token_ids, device=device)\n",
    "print(\"token_ids : \", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b70f6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref :  torch.Size([1, 50, 128])\n",
      "text_emb_128 :  torch.Size([1, 13, 128])\n",
      "learned_keys_50 :  torch.Size([50, 128])\n"
     ]
    }
   ],
   "source": [
    "ref = ref_encoder(logmel.to(device))\n",
    "print(\"ref : \", ref.shape)\n",
    "\n",
    "text_emb_128, learned_keys_50 = text_encoder(token_ids, ref, ref_key=None)\n",
    "print(\"text_emb_128 : \", text_emb_128.shape)\n",
    "print(\"learned_keys_50 : \", learned_keys_50.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f12dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_t :  tensor([0.2730], device='cuda:0') torch.Size([1])\n",
      "output :  torch.Size([1, 400, 100])\n"
     ]
    }
   ],
   "source": [
    "noisy_latents = torch.randn(B, 100*4, n_mels, device=device)\n",
    "time_t = torch.rand((B,), device=device)\n",
    "print(\"time_t : \", time_t, time_t.shape)\n",
    "\n",
    "output = vf_estimator(noisy_latents, time_t, text_emb_128, ref, learned_keys_50)\n",
    "print(\"output : \", output.shape) # B, secs*100, n_mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ae1ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 파라미터 수: 17,062,756\n",
      "전체 파라미터 수: 1,748,480\n",
      "전체 파라미터 수: 948,352\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in vf_estimator.parameters())\n",
    "print(f\"전체 파라미터 수: {total_params:,}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in text_encoder.parameters())\n",
    "print(f\"전체 파라미터 수: {total_params:,}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in ref_encoder.parameters())\n",
    "print(f\"전체 파라미터 수: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085a073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08277ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "class Adapter128to256(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(128, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 128)\n",
    "        return self.lin(x)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) 전체 파이프라인 예시\n",
    "# -------------------------------------------------\n",
    "class TTSVectorFieldPipeline(nn.Module):\n",
    "    def __init__(self, vocab_size: int, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # TextEncoder: 앞서 구현한 사양(임베딩 128, ConvNeXt x6, SA x4, Cross-Attn x2)\n",
    "        self.text_encoder = TextEncoder(vocab_size=vocab_size).to(device)\n",
    "\n",
    "        # ReferenceValueEncoder: logmel(100) → 128\n",
    "        self.ref_encoder = ReferenceValueEncoder(in_dim=100, out_dim=128).to(device)\n",
    "\n",
    "        # 128→256 어댑터\n",
    "        self.text_adapter = Adapter128to256().to(device)\n",
    "        self.ref_adapter  = Adapter128to256().to(device)\n",
    "\n",
    "        # VFEstimator: 144→256→(main blocks)→256→144\n",
    "        self.vf = VFEstimator(dim_in=144, dim_model=256, conv_hidden=1024, num_heads=4, Nm=4).to(device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward_once(\n",
    "        self,\n",
    "        text_ids: torch.Tensor,     # (B, T_text), long\n",
    "        logmel: torch.Tensor,       # (B, T_ref, 100), float\n",
    "        noisy_latents: torch.Tensor,# (B, T_ref, 144)\n",
    "        t_scalar: float = 0.5       # 예시 time t\n",
    "    ):\n",
    "        \"\"\"\n",
    "        1) ref_value = RefEncoder(logmel)  -> (B, T_ref, 128)\n",
    "        2) text_emb, learned_keys = TextEncoder(text_ids, ref_value)\n",
    "        3) text/ref 128→256 어댑터\n",
    "        4) VFEstimator(noisy_latents, t, text256, ref256, learned_keys_50)\n",
    "        \"\"\"\n",
    "        B, T_ref, _ = logmel.shape\n",
    "\n",
    "        # 1) Reference value\n",
    "        ref_value_128 = self.ref_encoder(logmel)  # (B, T_ref, 128)\n",
    "\n",
    "        # 2) Text encoder (speaker-adaptive text embedding + learned keys(50,128))\n",
    "        text_emb_128, learned_keys_50 = self.text_encoder(text_ids, ref_value_128, ref_key=None)  # (B, T_text, 128), (50,128)\n",
    "\n",
    "        # 3) 128 → 256\n",
    "        text_emb_256 = self.text_adapter(text_emb_128)  # (B, T_text, 256)\n",
    "        ref_value_256 = self.ref_adapter(ref_value_128) # (B, T_ref, 256)\n",
    "\n",
    "        # 4) VFEstimator forward\n",
    "        time_t = torch.full((B,), float(t_scalar), device=text_ids.device)\n",
    "        vf_pred = self.vf(\n",
    "            noisy_latents=noisy_latents,         # (B, T_ref, 144)\n",
    "            time_t=time_t,                      # (B,)\n",
    "            text_embed=text_emb_256,            # (B, T_text, 256)\n",
    "            ref_value=ref_value_256,            # (B, T_ref, 256)\n",
    "            learned_keys_50=learned_keys_50,    # (50, 128) → 내부에서 256로 proj\n",
    "            proj_text_to_dim=None,              # 이미 256로 변환했으므로 불필요\n",
    "            proj_ref_to_dim=None,\n",
    "        )\n",
    "        return vf_pred  # (B, T_ref, 144)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) 실제 사용 예시\n",
    "#    - 당신이 이미 계산한 logmel을 사용 (T, 100)\n",
    "#    - 텍스트 문자열 -> 배치 1로 토크나이즈\n",
    "#    - noisy_latents는 예시로 동일 T_ref 길이의 랜덤 텐서(144채널)\n",
    "# -------------------------------------------------\n",
    "def run_forward_example(text: str, logmel: torch.Tensor):\n",
    "    \"\"\"\n",
    "    text: 입력 텍스트\n",
    "    logmel: (T, 100)  # fbank.extract(audio, sr) 결과\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # (a) 토크나이저 & 배치화\n",
    "    tokenizer = SimpleCharTokenizer()\n",
    "    text_ids = tokenizer.batchify([text], device=device)  # (1, T_text)\n",
    "\n",
    "    # (b) logmel 배치화 및 dtype 맞춤\n",
    "    logmel = logmel.unsqueeze(0).to(device=device, dtype=torch.float32)  # (1, T_ref, 100)\n",
    "    B, T_ref, _ = logmel.shape\n",
    "\n",
    "    # (c) VF 입력용 noisy_latents 예시 (보통은 외부 노이즈 스케줄러/샘플러에서 옴)\n",
    "    noisy_latents = torch.randn(B, T_ref, 144, device=device)\n",
    "\n",
    "    # (d) 파이프라인 구성 & forward\n",
    "    pipe = TTSVectorFieldPipeline(vocab_size=tokenizer.vocab_size, device=device)\n",
    "    pipe.eval()\n",
    "\n",
    "    vf_pred = pipe.forward_once(\n",
    "        text_ids=text_ids,\n",
    "        logmel=logmel,\n",
    "        noisy_latents=noisy_latents,\n",
    "        t_scalar=0.5,   # 필요에 맞게 조정\n",
    "    )\n",
    "    print(\"vf_pred:\", vf_pred.shape)  # (1, T_ref, 144)\n",
    "    return vf_pred\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) 예시 실행 (당신이 이미 계산한 logmel을 그대로 사용)\n",
    "# -------------------------------------------------\n",
    "# 예:\n",
    "# from feature import TorchAudioFbank, TorchAudioFbankConfig\n",
    "# fbank = TorchAudioFbank(config=TorchAudioFbankConfig(sampling_rate=24000, n_mels=100, n_fft=1024, hop_length=256))\n",
    "# audio, sr = librosa.load(\"./test.wav\", sr=24000)\n",
    "# audio = torch.from_numpy(audio)\n",
    "# logmel = fbank.extract(audio, sr)  # (T, 100)\n",
    "\n",
    "# 이제 forward:\n",
    "# vf = run_forward_example(\"오늘 회의 주제 뭐였지?\", logmel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcafd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
